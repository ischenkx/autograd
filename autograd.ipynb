{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "outputs": [],
   "source": [
    "class Derivatives(object):\n",
    "    @staticmethod\n",
    "    def sum(tensors, grad):\n",
    "        return [np.ones(t.shape) * grad for t in tensors]\n",
    "\n",
    "    @staticmethod\n",
    "    def multiplication(tensors, g):\n",
    "        grad = []\n",
    "        for i, t1 in enumerate(tensors):\n",
    "            partial_grad = np.ones(t1.shape)\n",
    "            for j, t2 in enumerate(tensors):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                partial_grad *= t2\n",
    "            partial_grad *= g\n",
    "            grad.append(partial_grad)\n",
    "        return grad\n",
    "\n",
    "    @staticmethod\n",
    "    def division(tensors, grad):\n",
    "        if len(tensors) != 2:\n",
    "            raise Exception('unexpected amount of tensors for division (currently only two operands are supported)')\n",
    "\n",
    "        [f, g] = tensors\n",
    "        return [\n",
    "            grad / g,\n",
    "            -f * grad / (g * g)\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def negation(tensors, grad):\n",
    "        return [-1 * grad * np.ones(tensors[0].shape)]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "outputs": [],
   "source": [
    "class Tensor(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def as_expr(t):\n",
    "        if t.ntype == 'var':\n",
    "            return str(t.data)\n",
    "        if t.ntype == 'sum':\n",
    "            return f'({Tensor.as_expr(t.children[0])} + {Tensor.as_expr(t.children[1])})'\n",
    "        if t.ntype == 'mul':\n",
    "            return f'({Tensor.as_expr(t.children[0])} * {Tensor.as_expr(t.children[1])})'\n",
    "        if t.ntype == 'neg':\n",
    "            return f'-({Tensor.as_expr(t.children[0])})'\n",
    "        if t.ntype == 'div':\n",
    "            return f'({Tensor.as_expr(t.children[0])} / {Tensor.as_expr(t.children[1])})'\n",
    "    _id_generator = 0\n",
    "\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 autograd=True,\n",
    "                 children=None,\n",
    "                 derivative=None,\n",
    "                 dtype=np.float64,\n",
    "                 ntype=\"var\"):\n",
    "\n",
    "        if type(data) is not np.ndarray:\n",
    "            data = np.array(data, dtype)\n",
    "        else:\n",
    "            data = data.astype(dtype)\n",
    "\n",
    "        self.data = data\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = children\n",
    "        self.derivative = derivative\n",
    "        self.ntype = ntype\n",
    "        self.dtype = dtype\n",
    "        self.id = None\n",
    "        self._generate_id()\n",
    "\n",
    "    def _generate_id(self):\n",
    "        self.id = Tensor._id_generator\n",
    "        Tensor._id_generator += 1\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'type=\\'{self.ntype}\\' data={self.data}'\n",
    "\n",
    "    def __add__(self, other: 'Tensor'):\n",
    "        return Tensor(data=self.data + other.data,\n",
    "                      autograd=True,\n",
    "                      children=(self, other),\n",
    "                      derivative=Derivatives.sum,\n",
    "                      dtype=self.dtype,\n",
    "                      ntype='sum')\n",
    "\n",
    "    def __mul__(self, other: 'Tensor'):\n",
    "        return Tensor(data=self.data * other.data,\n",
    "                      autograd=True,\n",
    "                      children=(self, other),\n",
    "                      derivative=Derivatives.multiplication,\n",
    "                      dtype=self.dtype,\n",
    "                      ntype='mul')\n",
    "\n",
    "    def __neg__(self):\n",
    "        return Tensor(data=-self.data,\n",
    "                      autograd=True,\n",
    "                      children=(self,),\n",
    "                      derivative=Derivatives.negation,\n",
    "                      dtype=self.dtype,\n",
    "                      ntype='neg')\n",
    "\n",
    "    def __sub__(self, other: 'Tensor'):\n",
    "        a = self\n",
    "        b = -other\n",
    "        return b + a\n",
    "\n",
    "    def __truediv__(self, other: 'Tensor'):\n",
    "        return Tensor(data=self.data / other.data,\n",
    "                      autograd=True,\n",
    "                      children=(self, other),\n",
    "                      derivative=Derivatives.division,\n",
    "                      dtype=self.dtype,\n",
    "                      ntype='div')\n",
    "\n",
    "    # def cosine(self):\n",
    "\n",
    "    def topsort(self, blacklist=None):\n",
    "        if blacklist is None:\n",
    "            blacklist = {}\n",
    "        if self.id not in blacklist:\n",
    "            blacklist[self.id] = 0\n",
    "            yield self\n",
    "        if self.children is not None:\n",
    "            for child in self.children:\n",
    "                for n in child.topsort(blacklist):\n",
    "                    yield n\n",
    "\n",
    "    def accumulate(self, grad):\n",
    "        if self.grad is None:\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad\n",
    "\n",
    "    def backward(self):\n",
    "        if not self.autograd:\n",
    "            return\n",
    "\n",
    "        if self.grad is None:\n",
    "           self.grad = np.ones(self.data.shape, dtype=self.dtype)\n",
    "\n",
    "        for n in self.topsort():\n",
    "            if n.children is None:\n",
    "                continue\n",
    "            gradient = n.derivative([x.data for x in n.children], n.grad)\n",
    "            for child, grad in zip(n.children, gradient):\n",
    "                child.accumulate(grad)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.grad = None\n",
    "        for child in self.children:\n",
    "            child.zero_grad()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "outputs": [],
   "source": [
    "A = 2342343432423324233234\n",
    "B = 742324432"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.832398255368644e+68\n",
      "-3.790757525216277e+67\n"
     ]
    }
   ],
   "source": [
    "from torch import tensor\n",
    "\n",
    "a = tensor(A, dtype=float, requires_grad=True)\n",
    "b = tensor(B, dtype=float, requires_grad=True)\n",
    "d =  ( b* (a + (-b)) * a) * b\n",
    "\n",
    "d.backward()\n",
    "\n",
    "print(float(a.grad))\n",
    "print(float(b.grad))\n",
    "# print(c.grad)\n",
    "# print(d.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.832398255368644e+68\n",
      "-3.790757525216277e+67\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(A, dtype=np.float64)\n",
    "b = Tensor(B)\n",
    "d =  (b * (a + (-b)) * a) * b\n",
    "d.backward()\n",
    "print(a.grad)\n",
    "print(b.grad)\n",
    "# print(Tensor.as_expr(d))\n",
    "# print(c.grad)\n",
    "# print(d.grad)\n",
    "\n",
    "# for n in d.topsort():\n",
    "#     print(n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "outputs": [
    {
     "data": {
      "text/plain": "1.9917318911420165"
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.1077136540091577e+22 / 5.56156006205242e+21"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failure: -8153747712.0 -8053063680.0 74 74 81537477.12\n",
      "var\n",
      "stopped at 287\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "torch.set_warn_always(True)\n",
    "np.seterr('print')\n",
    "def test(ops, max_leaves, max_ops):\n",
    "    my_leaves = []\n",
    "    torch_leaves = []\n",
    "\n",
    "    for i in range(random.randint(1, max_leaves)):\n",
    "        val = random.randint(1, 4)\n",
    "        my_leaves.append(Tensor(val, dtype=np.float64))\n",
    "        torch_leaves.append(torch.tensor(val, dtype=torch.float64, requires_grad=True))\n",
    "\n",
    "    my_op = my_leaves[0]\n",
    "    torch_op = torch_leaves[0]\n",
    "\n",
    "    for i in range(random.randint(1, max_ops)):\n",
    "\n",
    "\n",
    "        leaf_index = random.randint(0, len(my_leaves) - 1)\n",
    "        op = random.choice(ops)\n",
    "        my_op = op(my_op, my_leaves[leaf_index])\n",
    "        torch_op = op(torch_op, torch_leaves[leaf_index])\n",
    "        if torch_op.detach().numpy() != my_op.data:\n",
    "            print('some failure!')\n",
    "\n",
    "\n",
    "    my_op.backward()\n",
    "    torch_op.backward()\n",
    "\n",
    "    for i in range(len(my_leaves)):\n",
    "        ml, tl = my_leaves[i].grad, torch_leaves[i].grad\n",
    "        if ml is None or tl is None:\n",
    "            if ml != tl:\n",
    "                return False\n",
    "            continue\n",
    "        tl = tl.numpy()\n",
    "\n",
    "        epsilon = max(abs(ml), abs(tl)) / 100\n",
    "        if abs(ml - tl) > epsilon:\n",
    "            print('failure:', ml, tl, len(my_leaves), len(torch_leaves), epsilon)\n",
    "            # print(Tensor.as_expr(my_op))\n",
    "            print(my_leaves[i].ntype)\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "OPS = [\n",
    "    lambda a, b: a * b,\n",
    "    lambda a, b: a + b,\n",
    "    lambda a, b: a - b,\n",
    "    lambda a, b: -a,\n",
    "    lambda a, b: a + (-b),\n",
    "    # lambda a, b: a / b\n",
    "]\n",
    "\n",
    "TESTS = 1000\n",
    "MAX_LEAVES = 100\n",
    "MAX_OPS = 1000\n",
    "\n",
    "for i in range(1000):\n",
    "    if not test(OPS, MAX_LEAVES, MAX_OPS):\n",
    "        print('stopped at', i + 1)\n",
    "        break\n",
    "else:\n",
    "    print('PASSED')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "outputs": [],
   "source": [
    "MB = [\n",
    "    [1,2,3,4,5],\n",
    "    [1,2,3,4,5],\n",
    "    [1,2,3,4,5],\n",
    "    [1,2,3,4,5],\n",
    "    [1,2,3,4,5]\n",
    "]\n",
    "\n",
    "MA = [\n",
    "    [2, 2, 2, 2, 2],\n",
    "    [2, 2, 2, 2, 2],\n",
    "    [2, 2, 2, 2, 2],\n",
    "    [2, 2, 2, 2, 2],\n",
    "    [2, 2, 2, 2, 2]\n",
    "]\n",
    "\n",
    "MC = [\n",
    "    [3,3,4,3,3],\n",
    "    [3,3,4,3,3],\n",
    "    [3,3,4,3,3],\n",
    "    [3,3,4,3,3],\n",
    "    [3,3,4,3,3]\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 36., 144., 432., 576., 900.],\n",
      "        [ 36., 144., 432., 576., 900.],\n",
      "        [ 36., 144., 432., 576., 900.],\n",
      "        [ 36., 144., 432., 576., 900.],\n",
      "        [ 36., 144., 432., 576., 900.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = tensor(MA, dtype=torch.float64, requires_grad=True)\n",
    "b = tensor(MB, dtype=torch.float64, requires_grad=True)\n",
    "c = tensor(MC, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "d = a * b\n",
    "e = b * c\n",
    "f = d * d * e\n",
    "\n",
    "f.backward(tensor([\n",
    "    [1,1,1,1,1],\n",
    "    [1,1,1,1,1],\n",
    "    [1,1,1,1,1],\n",
    "    [1,1,1,1,1],\n",
    "    [1,1,1,1,1],\n",
    "]))\n",
    "print(b.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 36. 144. 432. 576. 900.]\n",
      " [ 36. 144. 432. 576. 900.]\n",
      " [ 36. 144. 432. 576. 900.]\n",
      " [ 36. 144. 432. 576. 900.]\n",
      " [ 36. 144. 432. 576. 900.]]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(MA)\n",
    "b = Tensor(MB)\n",
    "c = Tensor(MC)\n",
    "\n",
    "d = a * b\n",
    "e = b * c\n",
    "f = d * d * e\n",
    "\n",
    "\n",
    "f.backward()\n",
    "print(b.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}